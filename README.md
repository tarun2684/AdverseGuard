# Adversarial ML Red Team Toolkit

ğŸ§  **Adversarial ML Red Team Toolkit** is a Python-based utility for testing machine learning models against common adversarial attacks in **computer vision** and **NLP** domains.  
It supports attacks such as **FGSM**, **PGD**, and **TextFooler**, helping you evaluate your modelâ€™s robustness under malicious perturbations.

---

## Features

- ğŸ” **Multiple attack methods**: FGSM, PGD, TextFooler (extendable to more).
- ğŸ“Š **Performance evaluation** before and after attack.
- ğŸ›  **Plug-and-play** with scikit-learn, PyTorch, or TensorFlow models.
- ğŸ“¦ **Customizable** attack parameters via CLI.
- ğŸ§ª Works on **image** and **text** datasets.

---

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/adversarial-ml-redteam.git
cd adversarial-ml-redteam

# Create and activate a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate   # On macOS/Linux
venv\Scripts\activate      # On Windows

# Install dependencies
pip install -r requirements.txt
